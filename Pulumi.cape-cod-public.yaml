encryptionsalt: v1:AiR3UYhLLnM=:v1:Q0tmuQ+UqDnv2UED:Vaku1iGHx8jIYc6cYUnD9ZFBW5H/Dw==
config:
  # This block is for meta about the deployment itself that may be reused in
  # later config settings.
  deployment:meta: &depmet
    # stage-name is used anywhere we need the deployment stage name and
    # should be values like "dev", "prod", etc. There is exact no exact
    # enumeration of values yet.
    stage-name: "dev"
  aws:region: us-east-2
  cape-cod:meta:
    glue:
      etl:
        - name: etl-gphl-cre
          key: glue/etl/etl_gphl_cre_alert.py
          srcpth: ./assets/etl/etl_gphl_cre_alert.py
        - name: etl-tnl
          key: glue/etl/etl_tnl_alert.py
          srcpth: ./assets/etl/etl_tnl_alert.py
        - name: etl-fastx
          key: glue/etl/etl_fasta_fastq.py
          srcpth: ./assets/etl/etl_fasta_fastq.py
        - name: etl-gphl-sequencing
          key: glue/etl/etl_gphl_sequencing_alert.py
          srcpth: ./assets/etl/etl_gphl_sequencing_alert.py
  cape-cod:swimlanes:
    private:
      # TODO: This is huge. way bigger than we need. For growth but also
      # cause we don't really know what address space we need yet. Adjust
      # as needed
      cidr-block: 10.0.0.0/16
      public-subnet:
        # public subnet is special and there will be one (for now) per
        # swimlane (vpc), the main purpose is currently for a NAT to
        # live in to allow egress to the internet for the instances in
        # private subnets
        cidr-block: 10.0.1.0/24
      private-subnets:
        - name: compute
          cidr-block: 10.0.2.0/24
          # if True, the private subnet's outgoing traffic will be
          # routed to the NAT gateway for egress to the internet
          egress-to-nat: True
      # NOTE: the apis section here will likely need to be re-worked as we
      #       move the api to a real host name, add certs, and all the
      #       things. for now just trying to get the stage name
      #       configurable from the root deployment:meta section
      api:
        dap:
          meta: *depmet
      compute:
        environments:
          - name: analysis
            # an AMI ID to use for each EC2 instance
            image: ami-05b2a3fcf0e46a742
            # a list of subnets that ec2 instances in the compute
            # environments live on
            subnets:
              - compute
            resources:
              # a list of instance types to be able to request
              instance_types: ["c4.large"]
              # the maximum number of vCPUs to have in the environment
              max_vcpus: 16
              # (optional) the desired number of vCPUs to have in the environment
              # desired_vcpus: 8
              # (optional) the minimum number of vCPUs to have in the environment
              # min_vcpus: 8
  cape-cod:datalakehouse:
    tributaries:
      - name: hai
        buckets:
          raw:
            name:
            crawler:
          clean:
            name:
            crawler:
              exclude:
              schedule: "0/5 * * * ? *"
              classifiers:
                - cape-csv-standard-classifier
        pipelines:
          data:
            etl:
              - name: tnl
                script: glue/etl/etl_tnl_alert.py
                prefix: tnl
                suffixes:
                  - xlsx
                pymodules:
                  - openpyxl==3.1.2
              - name: gphl-cre
                script: glue/etl/etl_gphl_cre_alert.py
                prefix: gphl-cre
                suffixes:
                  - docx
                pymodules:
                  - python-docx==1.1.2
              - name: gphl-sequencing
                script: glue/etl/etl_gphl_sequencing_alert.py
                prefix: gphl-sequencing
                suffixes:
                  - pdf
                pymodules:
                  - tabula-py==2.9.3
                  - pypdf==4.3.1
      - name: genomics
        buckets:
          raw:
            name:
            crawler:
          clean:
            name:
            crawler:
              classifiers:
        pipelines:
          data:
            etl:
              - name: fastx
                script: glue/etl/etl_fasta_fastq.py
                prefix: fastx
                suffixes:
                  - gz
                  - fasta
                  - fastq
                pymodules:
                  - pyfastx==2.1.0
                max_concurrent_runs: 5 # optional, 5 is the default
